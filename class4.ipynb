{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35b6b0e",
   "metadata": {},
   "source": [
    "# Scientific Programming: A Crash Course\n",
    "\n",
    "## Class 4 – The Scientific Stack\n",
    "\n",
    "Today we're going to start doing the *science* part of scientific programming. It won't be possible to cover particular types of analysis in any great depth, but I hope you will get a general idea of what stuff is available to you and where to start looking when you need to explore things further. By the end of this session, I hope you will be able to load some datasets and run some simple simulations. To do this, we are going to use some new packages that provide lots of functionality well beyond what you can easily achieve in Python alone. These packages are part of what is know as the \"scientific stack\" (i.e. the stack of packages that are widely used in science). The scientific stack is not strictly defined, but most people would consider at least the following packages to be core members:\n",
    "\n",
    "- [NumPy](https://numpy.org) – Arrays and numerical computation\n",
    "- [SciPy](https://scipy.org) – Linear algebra, optimization, signal processing\n",
    "- [Pandas](https://pandas.pydata.org) – Data frames\n",
    "- [Matplotlib](https://matplotlib.org) – Plotting and visualization\n",
    "\n",
    "However, here are many other packages that could also be considered part of the stack:\n",
    "\n",
    "- [NetworkX](https://networkx.org) – Graphs and network analysis\n",
    "- [Numba](http://numba.pydata.org) – Just-in-time compiler (make things faster)\n",
    "- [PyMC](https://docs.pymc.io) and [Bambi](https://bambinos.github.io/bambi/main/) - Bayesian statistical modelling\n",
    "- [Scikit-Learn](https://scikit-learn.org/stable/index.html) - Machine learning tools\n",
    "- [Scikit-Optimize](https://scikit-optimize.github.io/stable/) – Black box function optimization\n",
    "- [StatsModels](https://www.statsmodels.org/stable/index.html) – Traditional frequentist stats, including regression models\n",
    "- [SymPy](https://www.sympy.org/en/index.html) - Mathematics and symbolic computation\n",
    "- [TensorFlow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org) – Neural networks\n",
    "- [Xarray](https://docs.xarray.dev/en/stable/) - Efficient high-dimensional array handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3114aa7",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "\n",
    "The ordinary way to install Python packages is to use the `pip` tool. This downloads a package from the [Python Package Index (PyPI)](https://pypi.org) and installs it. It also automatically downloads and installs any other packages that your chosen package depends on – its \"dependencies.\" For example, `package_a` might depend on `package_b`, which depends on `package_c` and `package_d`. Using `pip` to install `package_a` will install all four of the packages. Here are a few useful commands (note, that these need to be used at the terminal, not in the Python interpreter or in a notebook):\n",
    "\n",
    "- `pip list` – list the packages you have installed\n",
    "- `pip show package_name` – get info about a particular package you have installed\n",
    "- `pip install package_name` – install a new package and its dependencies\n",
    "- `pip uninstall package_name` – uninstall a package (this does *not* remove its dependencies)\n",
    "\n",
    "However, installing packages can also get a little messy depending on how you set things up. If you are using Anaconda, for example, then it's best to use their own tool `conda` to install packages. This is similar to `pip` but it also provides more options to create separate **virtual environments**, isolated spaces in which you install a collection of packages. This allows you to maintain separate environments for separate projects. For example, in one project, you might be using version 1 of `package_x` and in another project you might be using version 2 of `package_x`. By maintaining separate environments, you can avoid potential conflicts. I don't want to go into more detail today about virtual environments, but they are extremely useful, and if you are planning to use Python in the long term, I would highly recommend that you start using them.\n",
    "\n",
    "In any case, if you are using Anaconda, then you should have everything installed already; by default, Anaconda includes the most widely-used packages in the scientific stack. It's also worth noting that packages can be managed and installed using Anaconda's GUI interface, although I haven't used this myself, so I'm not exactly sure how it works. Check out this link for more help: https://docs.anaconda.com/anaconda/navigator/getting-started/#navigator-managing-packages\n",
    "\n",
    "Anyway, to check that things are installed correctly, try running this code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import pandas\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c398edb",
   "metadata": {},
   "source": [
    "If any of the packages fail to import, you will need to install them first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ca8ce",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "NumPy (officially pronounced /nʌmpaɪ/ although many people call it /nʌmpi/) is *the* most fundamental package in the scientific stack. Whenever I need to do something mathy or sciency, NumPy is always the very first thing I import, so let's go ahead and do that right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca27afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae78aac",
   "metadata": {},
   "source": [
    "Note that I am importing `numpy` as `np` here. This means I am importing the package but giving it a different, shorter name, so that I don't have to keep typing `numpy` all the time. This convention of importing `numpy` as `np` is very common, so it's good to get into the habit.\n",
    "\n",
    "### Arrays\n",
    "\n",
    "Right, so what is NumPy anyway? NumPy provides support for array-based numerical computation, something that is not directly available in the core Python language. The closest thing in base-Python is the `list` data type, but NumPy arrays extend the `list` concept much further. If you are coming from Matlab or Julia, you should find NumPy arrays very familiar. There are also a lot of similarities with vectors and matrices in R.\n",
    "\n",
    "Let's make a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array( [1, 2, 3, 4, 5] )\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbf449",
   "metadata": {},
   "source": [
    "As you can see, an array basically looks like a list. In fact, to create this array I literally made a list first (recall that square brackets are used to make lists) and then I passed this list into the `np.array()` function to convert it into an array.\n",
    "\n",
    "So, what can I do with this array? Well, first, look what happens if I perform some basic mathematical operations with the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc529e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a62ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array - 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8684274",
   "metadata": {},
   "source": [
    "As you can see, when you perform some mathematical operation with a scalar (a single number), that operation is applied to all the numbers in the array simultaneously. If you wanted to achieve the same effect with base-Python, you'd have to do something a bit more awkward, perhaps involving a list comprehension like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1, 2, 3, 4, 5]\n",
    "print([x*10 for x in my_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f245426",
   "metadata": {},
   "source": [
    "You can also perform mathematical operations with two (or more) arrays, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([1, 2, 3, 4, 5])\n",
    "array2 = np.array([1, 10, 100, 1000, 1000])\n",
    "\n",
    "print(array1 * array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac0596",
   "metadata": {},
   "source": [
    "Do you see what happened? Each number in `array1` was multiplied by the corresponding number in `array2`. This is called **elementwise multiplication**. As you would expect, you can also do elementwise addition, subtraction, division, and exponentiation. The key thing to remember is that the arrays need to have the same length so that they can be [\"broadcast\"](https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm) together. Try writing some more examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fc94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4473e0aa",
   "metadata": {},
   "source": [
    "Arrays can have more than one dimension. Here's a two-dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(array_2d * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08522086",
   "metadata": {},
   "source": [
    "You don't need to create the array with indentation, as I have done here – I just did that to make the code more readable. A 2D array is like a list of lists, where the inner lists are all of the same length.\n",
    "\n",
    "Now let's multiply a 2D array and 1D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1d = np.array([10, 100, 1000])\n",
    "\n",
    "array_2d = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(array_1d * array_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853a54c",
   "metadata": {},
   "source": [
    "Do you see what happened? The elementwise multiplication is applied to each row. How would you do the same to each column – elementwise multiplication across the columns (hint: it's not very obvious, but you need a \"column array\").\n",
    "\n",
    "Arrays have several attributes that are sometimes useful to access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa106846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_2d.ndim) # Number of dimensions\n",
    "print(array_2d.shape) # Shape of the dimensions (n rows, n columns)\n",
    "print(array_2d.size) # Total number of elements / cells\n",
    "print(array_2d.dtype) # Data type of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfb71c",
   "metadata": {},
   "source": [
    "So, in this case, `array_2d` is a two-dimensional array with a 3×3 shape, and it contains nine 64-bit integers. One important thing to note about arrays is that all the elements need to be of the same data type; unlike regular Python lists, you cannot mix different types together. In this case, for example, all the values are integers. This allows NumPy to store and process the numbers very efficiently, which can be important in computationally intensive applications.\n",
    "\n",
    "Unlike regular Python, NumPy gives you much more flexibility over the data type. For example, if you needed to, you could create an array that uses 8-bit integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc724bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2e075",
   "metadata": {},
   "source": [
    "8-bit integers allow you to represent $2^8 = 256$ different values, specifically, the numbers -128 through +127. (Try putting some larger values in the array above; what happens? Do you understand why? This is called numerical overflow.) In many cases, 8-bit integers will be very limiting – usually we need to represent a much larger range of numbers. However, if you are doing some kind of computation where you only need small numbers, using 8-bit integers can be beneficial because they use less memory compared to the default 64-bit integers. Base-Python doesn't give you this level of control, but NumPy does.\n",
    "\n",
    "One example of where 8-bit integers are frequently used is in storing RGB (red, green, blue) color data – for example, each pixel in an image is stored as an RGB value. By using 8-bit integers, we can represent 256 shades of red, 256 shades of green, and 256 shades of blue. This means that $256^3 = 16$ million colors can be represented, with each pixel requiring just $8 \\times 3 = 24$ bits of data. If we were using 64-bit integers, each pixel would require $64 \\times 3 = 192$ bits, even if we limited ourselves to making the same 16 million color distinctions.\n",
    "\n",
    "In general, however, you don't need to worry about more exotic data types like 8-bit integers – the default ints and floats will be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e7fb8",
   "metadata": {},
   "source": [
    "### Array creation\n",
    "\n",
    "Rather than manually creating arrays by passing in a list, as we have been doing so far, there is often a function for creating the particular kind of array you need. For example, if you need an array of consecutive numbers, you can use `np.arange()` (note that this is array-range, the NumPy equivalent of `range()`, not the word *arrange* – for years I misread this function name as *arrange*!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.arange(0, 10)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a2545",
   "metadata": {},
   "source": [
    "Just like (almost) everything else in Python, ranges are inclusive:exclusive in NumPy. Which reminds me! NumPy has its own set of random functions within the `numpy.random` module. To create an array of 100 random ints from 0 to 9, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_nums = np.random.randint(0, 10, 100)\n",
    "print(rand_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8807ec6",
   "metadata": {},
   "source": [
    "Notice how this is different from Python's built-in `random.randint()`, which is weirdly inclusive:inclusive. The NumPy version of the `randint()` function is sensible; all the random integers generated here will be less than 10.\n",
    "\n",
    "Another common type of array you will need is an array of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a826bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_array = np.zeros( (4, 3) )\n",
    "print(zeros_array)\n",
    "print(zeros_array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c495b",
   "metadata": {},
   "source": [
    "When you use the `np.zeros()` function to create an array, you have to specify the desired shape; here, I asked for an array of zeros with the shape 4×3 (four rows, three columns). Since I didn't specify a data type, it defaulted to 64-bit floats.\n",
    "\n",
    "You might be wondering why you would ever want an array of zeros. Typically this is useful if you need to do some kind of counting and you need to initialize the counts at zero. For example, maybe you want to construct a simple neural network and you need to initialize the weights to zero.\n",
    "\n",
    "There is also `np.ones()` for creating an array of ones, and `np.full()` for creating an array filled with a particular arbitrary number. Try using them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11382426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02df15a1",
   "metadata": {},
   "source": [
    "Another very common kind of array you will want to create is a linearly-spaced sequence or \"linspace\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c902901",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.linspace(0, 1, 21)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fc266",
   "metadata": {},
   "source": [
    "So, here, I am creating an array of `21` evenly spaced numbers from `0` to `1`. This is often useful for generating the *x*-values of a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919340",
   "metadata": {},
   "source": [
    "### Array indexing\n",
    "\n",
    "Indexing an array is very similar to indexing a Python list. To demonstrate, I will  first create an array of the numbers 1–9 reshaped into a 3×3 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine = np.arange(1, 10).reshape((3, 3))\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c32e02",
   "metadata": {},
   "source": [
    "Indexing is done using square brackets, just like with regular Python lists, except now you can also index into multiple dimensions by separating each dimension with a comma. For example, say I wanted to access the value on the first row (index 0 – remember, counting is from zero) and the second column (index 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_of_nine[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6e788",
   "metadata": {},
   "source": [
    "You can think of the indices like coordinates. The two key things to remember are that indexing is from 0 and the row index is always specified first. Check that you can access all nine numbers by specifying the appropriate row and column indices.\n",
    "\n",
    "NumPy also permits slices, just like regular Python indexing, except the slicing is... you guessed it!... generalized to multiple dimensions. For example, say I wanted to extract the first two rows and all three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_of_nine[0:2, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed876116",
   "metadata": {},
   "source": [
    "This can be read as: extract row 0 up-to-but-not-including row 2, and column 0 up-to-but-not-including column 3. Play around with the slices to check you understand. Note also that if you leave one side of the slice blank, the start or end of the dimension is implied. For example, here I extract the middle column (all rows, column 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_of_nine[:, 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7a47e",
   "metadata": {},
   "source": [
    "Like Python lists, arrays are mutable. This means they can be updated with new values. Let's change the middle number to a zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a95f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine[1, 1] = 0\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe5f97",
   "metadata": {},
   "source": [
    "You can also modify an entire row or column with a single line of code. Here I will change the middle column to a five:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac85b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine[:, 1] = 5\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2cc0e",
   "metadata": {},
   "source": [
    "And now I will increment the middle row by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776157dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine[1, :] += 1\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d029e",
   "metadata": {},
   "source": [
    "### NumPy functions\n",
    "\n",
    "NumPy includes lots of common mathematical functions that are designed to work with arrays. For example, you can use `np.sum()` to sum an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine = np.arange(1, 10).reshape((3, 3))\n",
    "\n",
    "print(box_of_nine)\n",
    "print( np.sum(box_of_nine) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d60f9",
   "metadata": {},
   "source": [
    "`np.sum()` is an example of an aggregator function – a function that reduces a set of values down to a single value. Other examples would be `np.min()`, `np.max()`, and `np.mean()`. These allow you to apply the function along particular dimensions (or axes, as they are referred to). For example, we can sum along the 0th axis (i.e. the rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.sum(box_of_nine, axis=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9f0e2",
   "metadata": {},
   "source": [
    "or we can sum along the 1st axis (i.e. the columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.sum(box_of_nine, axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aef885",
   "metadata": {},
   "source": [
    "Try this out with other aggregator functions such as `np.min()` and `np.max()`. Make sure you try to predict what the result will be before running the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b3493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ff389e6",
   "metadata": {},
   "source": [
    "NumPy contains functions for all the common mathematical operations, including:\n",
    "\n",
    "- `np.abs()` – absolute value\n",
    "- `np.argmax()` – argmax\n",
    "- `np.argmin()` – argmin\n",
    "- `np.cos()` – cosine\n",
    "- `np.dot()` – dot product\n",
    "- `np.log()` – natural logarithm\n",
    "- `np.log10()` – base-10 logarithm\n",
    "- `np.log2()` – base-2 logarithm\n",
    "- `np.mean()` – mean\n",
    "- `np.median()` – median\n",
    "- `np.prod()` – product\n",
    "- `np.sin()` – sine\n",
    "- `np.sqrt()` – square root\n",
    "- `np.std()` – standard deviation\n",
    "- `np.tan()` – tangent\n",
    "- `np.var()` – variance\n",
    "\n",
    "and much much more... Many of these functions are also implemented as array methods as well, which are sometimes more convenient to use.\n",
    "\n",
    "Lastly, although I only showed examples of 1D and 2D arrays above, everything we've covered generalizes to *n* dimensions! If you're curious, go back through some of the examples and try playing with higher-dimensional arrays.\n",
    "\n",
    "If you want to learn more about NumPy, check out the user guide: https://numpy.org/doc/stable/user/ The two main reasons why you would want to use NumPy are (1) if you are doing something computationally intensive where efficiency matters (e.g. imaging data or neural nets), and (2) if you are doing something mathy that involves manipulation of vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa558b8",
   "metadata": {},
   "source": [
    "## SciPy\n",
    "\n",
    "SciPy, as the name suggests, is a package that contains many scientific functions. Here we'll just look at three quick examples, but it's good to know that many common algorithms are often just a quick import away.\n",
    "\n",
    "### Pearson correlation coefficient\n",
    "\n",
    "If you've studied any statistics before, you will be aware of the correlation coefficient, *r*, which tells you how correlated two variables are – that is, to what extent does one variable increase as another variable increases. The `pearsonr()` function is contained within the `stats` module of SciPy and can therefore be imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c4c86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec6ad1",
   "metadata": {},
   "source": [
    "To use the function you need to input two lists of numbers, where the *i*th number in one list corresponds to the *i*th number in the other list. In fact, you can pass in lists or arrays or other kinds of iterable object; SciPy is not particularly fussy about what type of object you pass in, as long as it can basically be interpreted as some kind of iterable of numbers. Let's try it out with some pretend data. Here is the number of ice creams I ate each day last week, alongside how high my happiness level was each day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02897aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of week:          M   T   W   T   F   S   S\n",
    "number_of_icecreams = [ 2,  0,  1,  1,  0,  3,  2]\n",
    "my_happiness_level  = [80, 50, 60, 55, 60, 99, 85]\n",
    "\n",
    "pearsonr(number_of_icecreams, my_happiness_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372b190",
   "metadata": {},
   "source": [
    "This returns a so called `PersonRResult` object, which tells us the correlation coefficient itself – in this case *r* = 0.93, ice cream seems to make me happy – and the corresponding p-value, which is less than 0.05, so we can conclude that ice cream is *significantly* related to my happiness level. If you need direct access to the actual numbers (e.g. for additional processing), they can be referenced like this (the relevant numbers are just attributes of the `PearsonRResult` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e11469",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pearsonr(number_of_icecreams, my_happiness_level)\n",
    "\n",
    "print( result.statistic )\n",
    "print( result.pvalue )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c87b3a",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d93ba8",
   "metadata": {},
   "source": [
    "Often our datasets contain clusters and we'd like an automated, objective way to identify those clusters. For example, let's say I taste lots of ice creams and rate each one on a 1–10 scale. Here are my rankings in the form of a simple list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c58448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "icecream_rankings = [\n",
    "     2., # banana\n",
    "     3., # bubblegum\n",
    "     4., # butterscotch\n",
    "     9., # cherry\n",
    "    10., # chocolate\n",
    "    10., # coffee\n",
    "     4., # mango\n",
    "     9., # mint\n",
    "    10., # pistachio\n",
    "     3., # raspberry\n",
    "     7., # vanilla\n",
    "     1., # watermelon\n",
    "]\n",
    "\n",
    "print(icecream_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7f921",
   "metadata": {},
   "source": [
    "Note that I split the list across multiple lines and added a comment to each line to make a note of which flavor each number represents, but of course these comments are not strictly necessary. Indeed, when we print the list, we just see it as a flat list of numbers.\n",
    "\n",
    "Now, let's say we know in advance that there are basically two clusters in these rankings: ice cream flavors that I like (which get a relatively high score) and flavors I don't like (which get a relatively low score). We can use [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) to help us categorize all the flavors into the good ones and bad ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "centroids, clusters = kmeans2(icecream_rankings, 2)\n",
    "print(centroids)\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ab45c",
   "metadata": {},
   "source": [
    "The `kmeans2()` function takes two inputs: a set of numbers (in this case, `icecream_rankings`) and a specified number of clusters to look for (in this case `2`). The function returns two things: the clusters themselves and the \"centroids\" (essentially the average scores) of the clusters. The clusters are arbitrarily represented as `0` and `1` (the zeros may represent the good flavors or the bad ones – it's arbitrary). The order of the zeros and ones corresponds to the order of the flavors in the original list, so in this case I can see, for example, that banana, bubblegum, and butterscotch were classified into the bad cluster (with an average score of 2.8), while cherry, chocolate, and coffee were classified into the good cluster (with an average score of 9.1).\n",
    "\n",
    "Try increasing the expected number of clusters to 3. Is the k-means algorithm able to identify three distinct categories of flavor? And if you're feeling adventurous, try writing a function to print the good and bad flavors by automatically matching up the zeros and ones to the relevant flavor names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac6b6d",
   "metadata": {},
   "source": [
    "### Function optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d234464",
   "metadata": {},
   "source": [
    "Sometimes we know the mathematical relationship between two variables, *x* and *y*, but we don't know what the best value of *x* is that maximizes (or minimizes) *y*. For example, maybe we have a good mathematical understanding of how the cream–to-ice ratio of ice cream relates to quality, and we want to find the ideal ratio that maximizes quality. In these cases, we can express the mathematical relationship as a function and then optimize that function using an optimizer. Here's an example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fdf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_icecream(ratio):\n",
    "    return 3 * ratio ** 4 - 2 * ratio + 1\n",
    "\n",
    "print( good_icecream(0.3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3def5b",
   "metadata": {},
   "source": [
    "So, if you use a 30% ratio (`0.3`), this results in quality of 0.4243. To find the ideal ratio, we will import the `minimize()` function from SciPy's `optimize` module, and then \"minimize\" our \"objective function\" `good_icecream()`. To use the `minimize()` function, we also have to specify an initial guess (in this case, we'll guess `0.5`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(good_icecream, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f8ca7",
   "metadata": {},
   "source": [
    "The optimizer starts by checking the quality when the ratio is 0.5 (the initial guess) and then it gradually increases and decreases this number in some clever ways until it finds the ideal value. The key thing to note in the output is the last line, `x`, which tells us the *x* value (i.e. the ratio) that resulted in the best quality, so a 55% ratio works best. (N.B. technically our objective function expresses the badness of the ice cream – generally, in optimization problems, we are seeking to minimize the objective function, so usually we express things in terms of minimizing rather than maximizing – we are looking for the ratio that minimizes badness)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477a83d",
   "metadata": {},
   "source": [
    "SciPy is a huge package that contains many other scientific functions, objects, and constants. In general, it's always a good idea to use the functions provided in packages rather than writing them manually yourself, since such implementations are usually well tested and optimized. For a more comprehensive list of stuff contained in SciPy, check out the [user guide here](https://docs.scipy.org/doc/scipy/tutorial/index.html#user-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca06ae0",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "We briefly encountered Pandas in the last class when we were loading CSV files. Pandas is a package for representing data frames – a concept that will be very familiar to you if you are coming from the R world.\n",
    "\n",
    "What is a data frame? A data frame is essentially a table or spreadsheet. It shares some similarities with 2D arrays, like those we've just been working with above; however, data frames are geared towards data analysis rather than numerical computations. Unlike arrays, data frames have headers, and different data types (ints, strings, Booleans) can be freely mixed together.\n",
    "\n",
    "Let's create a data frame from the `example_data.csv` that we used in the last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('example_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29270a4",
   "metadata": {},
   "source": [
    "First, similar to NumPy, the conventional way to import Pandas is `as pd`. Again, this is so that we don't have to keep typing `pandas` all the time. I then used the `pd.read_csv()` function to load the CSV file into a data frame. Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1af30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae50a2",
   "metadata": {},
   "source": [
    "As you can see, this gives us a nice overall summary of the dataset. There are four columns and 15,360 rows. There is also an extra \"column\" on the left which numbers the rows from 0 to 15359, and an extra \"row\" at the top which labels the columns – but these are not true rows and columns, just headers that describe the main content of the data frame. Notice also that we only see the first five rows and the last five rows. If we printed the whole data frame it would be massive. Looking at just the first and last rows is often pretty informative. For example, here we can see that there appear to be 240 subjects.\n",
    "\n",
    "The principle of \"tidy data\" that I mentioned in the last class says that each column should represent a \"variable\" (not in the coding sense, but in the statistical/experimental sense), and that each row should represent an observation. It is reasonable to assume, then, that each of the rows here represents a single trial, and that the subject was either correct or incorrect on each trial (represented as `0` or `1`). We also get the sense that there are some different conditions – different test types and category systems. Let's find out what values are present in these two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df['test_type'].unique() )\n",
    "print( df['category_system'].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a159b",
   "metadata": {},
   "source": [
    "Here, the brackets allow us to index each of the columns (first the `test_type` column and then the `category_system` column), and then calling the `.unique()` method shows us the unique values that exist in those columns. So, now we can see that this experiment appears to have a 2×3 design – there are two test types and three category systems. Another question we might have is, is this a between-subject or within-subject design? To answer that, we could isolate the data for just one of the participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('subject==1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd712",
   "metadata": {},
   "source": [
    "Note that this effectively creates a new data frame with only the data from subject 1 (technically, I don't think this creates a new data frame completely; rather, we get a \"view\" into a portion of the original data frame). Based on this, it looks like the experiment is fully between-subject: An individual subject does one of the two test types (this participant did `production`) and one of the three category systems (this participant did `size`). We also see here that the participant did 64 trials.\n",
    "\n",
    "One quick thing to note about the syntax here. `'subject==1'` is a string. This looks a bit awkward, but imagine you instead wrote `subject == 1` (the variable `subject` is `1`) or `'subject' == 1` (the string `'subject'` is 1). These conditions would evaluate as `False` and the query would not be interpretable. When you use the `.query()` method, you express the query as a string and Pandas then parses and interprets that string as some conditional statement pertaining to the data frame. This is different from R, which seems to magically figure out whether something is a variable or a data frame header (I never understood how this works – if someone could enlighten me, I would be grateful!).\n",
    "\n",
    "The next thing we might wonder is how accurate this participant was. First, let's assign the participant's subset of the data frame to a variable for convenient access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1 = df.query('subject==1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9460",
   "metadata": {},
   "source": [
    "and then we can simply sum the `correct` column to see how many of the trials (out of 64) were correct (the sum of all the zeros and ones effectively just counts the number of ones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1['correct'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee5fe1",
   "metadata": {},
   "source": [
    "Let's express that as a proportion instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1['correct'].sum() / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d4810",
   "metadata": {},
   "source": [
    "An alternative way to get this answer would be to take the mean of the column. Taking the mean of a bunch of zeros and ones is mathematically equivalent to calculating the proportion of trials that were correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac72cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1['correct'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88167f",
   "metadata": {},
   "source": [
    "Great, so this participant is 86% accurate. Is that good or bad? To answer this we need to look at the participant's performance in the context of the other participants, and it will be easier to do that with some visualizations, so we will return to this dataset tomorrow.\n",
    "\n",
    "But before we move on, if you want to learn more about Pandas, a good place to start is the user guide: https://pandas.pydata.org/docs/user_guide/ There's also a very handy cheat sheet here: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf If you're like me and you can't remember any of the commands, print out the cheat sheet and stick it on the wall behind your computer screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b046a",
   "metadata": {},
   "source": [
    "## Simulation, simulation, simulation!\n",
    "\n",
    "I love simulations!\n",
    "\n",
    "Whenever I encounter a new problem that I don't really understand, my first instinct is to simulate it. I often find that I understand something so much better when I try to code it up as a computational simulation. \"Computational simulation\" sounds really fancy and advanced, but the truth is that simulations are often super simple – it's just playing around with some numbers.\n",
    "\n",
    "Recently, I listened to a podcast about the [birthday paradox](https://en.wikipedia.org/wiki/Birthday_problem). If you haven't heard about it before, the birthday paradox says that, if you have a room with 23 people, there's a 50-50 chance that two of the people will share the same birthday. It's described as a \"paradox\" because it naturally feels very counterintuitive. It feels like you need way more than 23 people in a room for two of them to share a birthday. After listening to the podcast, I still didn't have a good intuition for it, so I decided to code up a simulation.\n",
    "\n",
    "The first thing we need is a way to represent birthdays. We could try to use dates, like 16th December, etc., but this would involve lots of awkward strings and exceptions for months that have fewer than 31 days. There's a much simpler option. We only need to represent each day of the year, so we can just use the numbers from 1 to 365. So, `1` represents 1st January and `365` represents 31st December. Easy!\n",
    "\n",
    "Now, let's write some code to generate random birthdays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_birthdays(n_people):\n",
    "    return np.random.randint(1, 366, n_people)\n",
    "\n",
    "print(generate_birthdays(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d290b",
   "metadata": {},
   "source": [
    "Here I've created a function that generates birthdays for *n* people, and then I called the function with *n* set to 10 to simulate a room of ten people. Check that you understand how the function works. I think it's pretty cool that you basically only need one line of code to simulate a room full of people and their birthdays. The best simulations are really simple like this – just numbers.\n",
    "\n",
    "Now we need a function to determine if two of the birthdays are the same. Here's what I came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63de9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_birthdays_are_same(birthdays):\n",
    "    for i in range(len(birthdays)):\n",
    "        for j in range(i + 1, len(birthdays)):\n",
    "            if birthdays[i] == birthdays[j]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "room_full_of_people = generate_birthdays(10)\n",
    "print( two_birthdays_are_same(room_full_of_people) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be389749",
   "metadata": {},
   "source": [
    "Here I first used the `generate_birthdays()` function to generate a room full of people – basically just a list of random numbers between 1 and 365, and then I used the `two_birthdays_are_same()` function to check if any of the numbers are the same (i.e. if two people have the same birthday). Try running the code a few times; sometimes it will be `True` and sometimes `False`.\n",
    "\n",
    "Now we get to the simulation part. We want to estimate the probability that two people share a birthday in a room with *n* people, so all we have to do is generate, say, 1000  rooms with a certain number of people and see what proportion of the time there is a shared birthday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of_shared_bithday(n_people, n_sims=1000):\n",
    "    count = 0\n",
    "    for _ in range(n_sims):\n",
    "        room_full_of_people = generate_birthdays(n_people)\n",
    "        count += two_birthdays_are_same(room_full_of_people)\n",
    "    return count / n_sims\n",
    "\n",
    "print(probability_of_shared_bithday(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d959b",
   "metadata": {},
   "source": [
    "You should get an answer like 0.11 – there's an 11% chance that two people will share a birthday if you have a room with 10 people. Study the function and make sure you understand everything. What happens when there's 23 people? Recall that the `two_birthdays_are_same()` function returns `True` or `False`, so what's happening on line five with the `+=`?\n",
    "\n",
    "Finally, let's put all this code together and compute the probability for different room sizes – a room containing two people all the way to a room containing 100 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa217bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_birthdays(n_people):\n",
    "    return np.random.randint(1, 366, n_people)\n",
    "\n",
    "def two_birthdays_are_same(birthdays):\n",
    "    for i in range(len(birthdays)):\n",
    "        for j in range(i + 1, len(birthdays)):\n",
    "            if birthdays[i] == birthdays[j]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def probability_of_shared_bithday(n_people, n_sims=1000):\n",
    "    count = 0\n",
    "    for _ in range(n_sims):\n",
    "        room_full_of_people = generate_birthdays(n_people)\n",
    "        count += two_birthdays_are_same(room_full_of_people)\n",
    "    return count / n_sims\n",
    "\n",
    "probs = [ probability_of_shared_bithday(n) for n in range(2, 101) ]\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c49ce",
   "metadata": {},
   "source": [
    "This will take a few seconds to run, but you should get a list of probabilities that gradually approach 1. It will be much easier to interpret these numbers with a plot. To make this plot, I'm going to use the Matplotlib package, which we'll look at in more detail tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3aa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(2, 101), probs)\n",
    "plt.axvline(23, color='gray', linestyle='--')\n",
    "plt.xlabel('Number of people in the room')\n",
    "plt.ylabel('Probability that two people share a birthday')\n",
    "plt.xlim(0, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e4586",
   "metadata": {},
   "source": [
    "As you can see, once you have 23 people in a room, the probability that two of them share a birthday hits 50%! QED.\n",
    "\n",
    "Of course, if you're a mathematician, you would just do a bunch of boring math stuff to calculate the probability exactly, but the wonderfully liberating thing about simulations is that you don't need to know the math! Moreover, there are some problems where the math is so complicated that you can't produce an analytic solution anyway, making simulation the only viable path to an (approximate) solution.\n",
    "\n",
    "### Activities\n",
    "\n",
    "1. Now that we've simulated the solution to the birthday paradox, does it feel more obvious why surprisingly few people are needed for two people to share a birthday?\n",
    "\n",
    "2. Currently, we're only showing results for two or more people in a room. What happens if there is just one person in a room or no people? Obviously, the probability would be zero, but how can we include this information on the plot?\n",
    "\n",
    "3. Compare our plot to the one on [the Wikipedia page](https://en.wikipedia.org/wiki/Birthday_problem). Our plot is not very smooth in comparison to the Wikipedia one. Why is this and how can we improve it?\n",
    "\n",
    "4. Technically, we didn't factor in leap years – there are some mythical people who were born on February 29th. How would you modify the code to account for this?\n",
    "\n",
    "5. Think about a problem in your own research where a simulation might be useful. Sketch out some ideas about how you'd approach the problem and try to code it up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
