{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35b6b0e",
   "metadata": {},
   "source": [
    "# Scientific Programming: A Crash Course\n",
    "\n",
    "## Class 5 – Visualization and Analysis\n",
    "\n",
    "One of the most important (but often overlooked) skills in science is being able to communicate your work to others. And, in my opinion, the absolute best way to communicate your work is with pictures. As the old saying goes, \"a picture is worth a thousand words\". There are many specialist packages for certain types of illustration – technical diagrams, brain scans, network structures, etc. – but the most common thing we need to do is create various types of plot to show the results of our data analyses. There are at least a few different options here. [Matplotlib](https://matplotlib.org), which we'll use today, is the most comprehensive and widely-used package, but another good option is [Seaborn](https://seaborn.pydata.org), which is a bit more modern, although more limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65fdc8",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "\n",
    "Matplotlib is the most widely-used plotting library in Python and a key component of the scientific stack. If you're coming from the R world, it is roughly equivalent to Ggplot. Matplotlib allows you to create all sorts of plots: the obvious ones, like bar plots, scatter plots, and line plots, but also the less obvious ones like violin plots, heatmaps, and timelines.\n",
    "\n",
    "Let's jump right in and import Matplotlib (we'll also import NumPy and Pandas as well, since we'll use them throughout this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da903c78",
   "metadata": {},
   "source": [
    "Like NumPy and Pandas, there is a conventional way that Matplotlib is imported. Matplotlib includes other modules for dealing with lower-level stuff like shapes and text, but 95% of the time we just need the `pyplot` module, so the convention is to import just that module and call it `plt`.\n",
    "\n",
    "Before we start plotting the some real data, let's look first at some more basic examples to get the general idea. First, let's start with a simple line plot. This type of plot is so basic, Matplotlib literally just calls the relevant function `plot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81454038",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 101)\n",
    "y = np.arange(1, 101)\n",
    "\n",
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c27de4",
   "metadata": {},
   "source": [
    "Here I created two sets of values, `x` and `y`, both of which are just the numbers one through one hundred. I then plotted `x` against `y`. To make this fake data look a little more interesting, let's add some random noise to the `y` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 101)\n",
    "y = np.arange(1, 101)\n",
    "y += np.random.randint(0, 5, 100)\n",
    "\n",
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff6ab0",
   "metadata": {},
   "source": [
    "If you're a little unsure what's happening here in terms of the data generation, try taking it step by step (what do `x` and `y` look like, how are we generating and adding the noise?). To make this plot a little more informative, we should add some *x*- and *y*-axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed093e",
   "metadata": {},
   "source": [
    "And... that default shade of blue is pretty boring..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "plt.plot(x, y, color='hotpink');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b6ab3",
   "metadata": {},
   "source": [
    "Another way we might want to plot this data is with a scatter plot. A scatter plot is like a line plot, except the points are not joined together with lines. A scatter plot usually makes sense when the points are not intrinsically ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "plt.scatter(x, y, color='hotpink');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df87f8",
   "metadata": {},
   "source": [
    "Let's make another scatter plot that looks a bit more realistic – we'll make the noise normally distributed. To do that, we'll use the `np.random.normal()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ad962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "y = x * 2 + np.random.normal(0, 0.2, 100)\n",
    "\n",
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "plt.scatter(x, y, color='hotpink');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9ee43",
   "metadata": {},
   "source": [
    "Finally, let's overlay a simple linear regression line on this plot. To do this, we'll use NumPy's `polyfit()` function to determine the best fitting regression line (represented as an intercept and slope) and then we'll draw that line on the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "y = x * 2 + np.random.normal(0, 0.2, 100)\n",
    "\n",
    "β, α = np.polyfit(x, y, 1) # fit regression line - slope and intercept\n",
    "y_predicted = α + β * x # y-values predicted by the regression line\n",
    "\n",
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "plt.scatter(x, y, color='hotpink')\n",
    "plt.plot(x, y_predicted, color='black', linewidth=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761febb",
   "metadata": {},
   "source": [
    "Try playing around with the colors and styles. Need some color inspiration? [Check this page for all the standard color names you can use.](https://www.w3schools.com/cssref/css_colors.asp) Can you change the circular points into squares or triangles? Can you make the regression line dashed instead of solid. Do some googling to find out how it's done.\n",
    "\n",
    "A great place to start whenever you have some new data is to plot a histogram to get an overall sense of the distribution of the data points. So it's worth taking a quick moment to see how it's done in Matplotlib. As you'll see, it's super easy, so you have no excuses for not plotting histograms of your data! First, I'll generate 1000 random numbers that are normally distributed with a mean of 0 and a standard deviation of 1, and then I'll use `plt.hist()` to make the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37492ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.random.normal(0, 1, 1000)\n",
    "\n",
    "plt.hist(values, color='indianred', bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8e91e",
   "metadata": {},
   "source": [
    "The `bins` argument allows you to control how many \"bins\" the datapoints are categorized into, which can give you a more granular view. Try making it bigger and smaller.\n",
    "\n",
    "Okay, now that we've played around with some a fake data, I hope you've got the general idea of how things work. Basically, you pass your data into one of the plotting functions – like `plt.plot()`, `plt.scatter()`, or `plt.hist()` – and you set some colors and labels, and hey presto, you have a plot! Of course, there are **a lot** more options for further customization. To see lots more example plots, and the code used to generate them, check the Matplotlib gallery here: https://matplotlib.org/stable/gallery/index\n",
    "\n",
    "## Dataset 1\n",
    "\n",
    "Let's go back to the dataset that we briefly worked with in the previous class. First, a quick reminder of what the dataset looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57972b70",
   "metadata": {},
   "source": [
    "There are four columns representing four different variables (I mean the word \"variable\" in the statistical sense, not the computing sense – i.e. things that are measured or recorded in the experiment), and there are 15,360 rows, each representing an individual experimental trial. The overall structure of the experiment is 240 subjects who do either a production test or a comprehension test, and they are tested on either a size, angle, or both category system. Finally, the `correct` variable/column records whether the trial subject was correct (`1`) or incorrect (`0`) on that trial.\n",
    "\n",
    "\n",
    "\n",
    "First, I just want to look at the results from the production test type, so let's isolate those trials first into a new data frame which we'll assign to the variable `production_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5615c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df = df.query('test_type==\"production\"')\n",
    "production_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42424f6e",
   "metadata": {},
   "source": [
    "As you can see, we now just have the 120 subjects who did the production test type. Next, I want to calculate accuracy for each of the category systems; to do this, we'll use the data frame's `.groupby()` method to group the data by category system and then calculate the mean of the `correct` column for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_condition = production_df.groupby('category_system')['correct'].mean()\n",
    "accuracy_by_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c9272",
   "metadata": {},
   "source": [
    "Right away we see that accuracy is highest in the `angle` condition and lowest in the `both` condition. Let's make a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_condition = accuracy_by_condition.sort_values(ascending=False) # sort in descending order\n",
    "\n",
    "plt.ylim(0, 1) # make the y-axis go from 0 to 1\n",
    "plt.xlabel('Category system')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.bar(accuracy_by_condition.index, accuracy_by_condition);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7864f2",
   "metadata": {},
   "source": [
    "Bar plots are terrible! You should almost never use them. [#barbarplots](http://barbarplots.github.io)! Let's do better by making a violin plot, which will show us not only the central tendency of the three conditions, but also their distributions. To make a violin plot, we need to organize the data a little bit so that we have subject-level accuracy scores for each of the category systems. To do that I will further subset the `production_df` into a separate data frame for each category system, and then I'll compute subject-level accuracy using `.groupby()` (we did something similar yesterday for just one subject)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a separate dataframe for each category system by\n",
    "# subsetting the main dataframe\n",
    "angl_df = production_df.query('category_system==\"angle\"')\n",
    "size_df = production_df.query('category_system==\"size\"')\n",
    "both_df = production_df.query('category_system==\"both\"')\n",
    "\n",
    "# group the dataframes by subject and then calculate the\n",
    "# accuracy per subject\n",
    "angl_accuracy_by_subject = angl_df.groupby('subject')['correct'].mean()\n",
    "size_accuracy_by_subject = size_df.groupby('subject')['correct'].mean()\n",
    "both_accuracy_by_subject = both_df.groupby('subject')['correct'].mean()\n",
    "\n",
    "# put all these by-subject accuracy scores together in one\n",
    "# list\n",
    "data = [angl_accuracy_by_subject, size_accuracy_by_subject, both_accuracy_by_subject]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52363508",
   "metadata": {},
   "source": [
    "Print out the variables if you're unsure what's happening at each step. Finally, let's make the violin plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(data, showmeans=True, showextrema=False)\n",
    "plt.xlabel('Category system')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([1, 2, 3], labels=['Angle', 'Size', 'Both'])\n",
    "plt.ylim(0, 1); # make the y-axis go from 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e64378",
   "metadata": {},
   "source": [
    "Great! Now we can get a sense of how much variation there is across participants within each condition, which we couldn't get from the bar plot. In this case, subjects who were tested on the angle system are very consistent – pretty much all of them have high accuracy scores; subjects who learned the size system are not only worse (on average) but also more variable – some did well and some did poorly; subjects who learned the both system are even more variable. Looking at the full distribution is always much more informative than just looking at the means (shown here with the dark blue lines) or other summary statistics, so you should get into this habit right away!\n",
    "\n",
    "The plot seems to show that people who learned the angle category system were more accurate than people who learned the size category system. But could we have gotten these results simply by chance? To answer these kinds of questions, we need statistics, which is beyond the scope of what I want to cover in this course. But to give you a quick example, here's how you could run a t-test to evaluate whether, for example, subjects in the angle condition were significantly better that those in the size condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c10c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(angl_accuracy_by_subject, size_accuracy_by_subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747f1a4",
   "metadata": {},
   "source": [
    "The p-value is less than 0.05, so yes indeed this does seem to be the case. To do this, I used the `ttest_ind()` function from SciPy (independent samples t-test), but you'll typically want to do more advanced analyses than this, for which there are more appropriate packages (e.g. the [Statsmodels](https://www.statsmodels.org/stable/index.html) package) or indeed more appropriate languages, like R.\n",
    "\n",
    "Using the code above, you should be able to make the equivalent plot for the comprehension data, which we previously ignored by extracting only the production test results. Try building the plot in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3596b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7877a39f",
   "metadata": {},
   "source": [
    "There are many more options for customizing the plot further. For reference, this is what the final plot looked like in my paper. You'll see that I added illustrations of the three category systems to make the plot easier to understand, and I also added a dashed line at 0.25 to show chance-level performance.\n",
    "\n",
    "![title](images/prod_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff7df6",
   "metadata": {},
   "source": [
    "## Dataset 2\n",
    "\n",
    "Let's switch now to anther dataset to get some more practice. This data is in `example_data2.csv`, so first we'll open the CSV file and print it to get a general sense of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c060d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example_data2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ae9b8",
   "metadata": {},
   "source": [
    "Can you answer the following questions? You might have to do a little bit more digging into the data to get the right answers.\n",
    "\n",
    "1. How many subjects are there?\n",
    "\n",
    "2. How many conditions are there?\n",
    "\n",
    "3. How many trials did each subject do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914806f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "740058db",
   "metadata": {},
   "source": [
    "This dataset is from an eye tracking experiment. Participants have to look at a word and we record which part of the word they look at – the \"landing position\", which is measured in pixels. Some participants were in the \"left\" condition where we expect them to look at the left part of the word, and some participants were in the right condition where we expect them to look at the right part of the word.\n",
    "\n",
    "To get a general sense of whether this hypothesis is true, we could calculate the average landing position in each condition. Write some code in the cell below to get these two numbers. For reference, the correct numbers should be 97.545887 in the left condition and 129.512789 in the right condition. Can you reproduce these numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730810c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ed66ef",
   "metadata": {},
   "source": [
    "As I mentioned before, just looking at summary statistics, like means, can be misleading, so it's essential to **always plot your data** (and, where possible, always look at the distribution – not just the central tendency). For this dataset, a density plot will work very nicely. To help us create this kind of plot, we'll use the `gaussian_kde()` function from SciPy. Study the code below to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7146e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "x = np.linspace(0, 252, 1000)\n",
    "\n",
    "left_positions = df.query('condition==\"left\"')['landing_position']\n",
    "right_positions = df.query('condition==\"right\"')['landing_position']\n",
    "\n",
    "y_left = gaussian_kde(left_positions).pdf(x)\n",
    "y_right = gaussian_kde(right_positions).pdf(x)\n",
    "\n",
    "plt.plot(x, y_left, color='cadetblue')\n",
    "plt.plot(x, y_right, color='crimson')\n",
    "\n",
    "plt.xlabel('Landing position (in pixels)')\n",
    "plt.xlim(0, 252)\n",
    "x_ticks = []\n",
    "for boundary in range(0, 253, 36):\n",
    "    plt.axvline(boundary, color='lightgray', zorder=0)\n",
    "    x_ticks.append(boundary)\n",
    "plt.xticks(x_ticks);\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ab266",
   "metadata": {},
   "source": [
    "Was our hypothesis correct? Is there a significant difference between conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f130a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77302cae",
   "metadata": {},
   "source": [
    "It's also really important to look at the data from each individual subject – maybe some subjects are being weird for some reason, or maybe there are clusters of subjects who tend to behave in similar ways. In the cell below, try to develop the above code to produce a density plot for each individual subject, not just the overall data from each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec09e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adf272f9",
   "metadata": {},
   "source": [
    "For reference, here's what the final figure looked like in my paper:\n",
    "\n",
    "\n",
    "![title](images/landing_pos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275373af",
   "metadata": {},
   "source": [
    "##  Lastly: A Note on Notebooks!\n",
    "\n",
    "During this course, we have mainly done all our coding within a Jupyter notebook. However, this is not the only way to use Python. You can also create Python scripts (`.py` files), which can be run directly, or use the interpreter. What do you think of the notebook format? Have you had any issues with it? Have you used something like this before in another language?\n",
    "\n",
    "Personally, I don't *love* notebooks, although I understand why many people do. The really nice thing is that you can mix text and code together. This makes notebooks very useful in an educational context – like this class – because we can mix code with explanations. Notebooks can also be useful in a scientific context because they allow you to clearly document your thought processes as you explore some data. For example, you can create a notebook that shows all the steps you took in analyzing some data, and then you can share this document with your PhD supervisor or publish it as supplementary material alongside a journal article. Another really nice feature is that all the plots you generate in kept alongside the code that generated them, so it's easy to remember how you created each plot.\n",
    "\n",
    "However, in my experience, the notebook style of coding can also sometimes get a bit messy and confusing for everyday programming, mostly because it's difficult to keep track of the current \"state\" of the underlying interpreter. For example, let's say we run the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_special_number = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290df6eb",
   "metadata": {},
   "source": [
    "Then, maybe we do some calculations with this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = my_special_number ** 2 + 100\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65135cec",
   "metadata": {},
   "source": [
    "Okay, great! The answer is 149. Now I run some more code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db28765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "magical_constant = 64\n",
    "my_special_number = 10\n",
    "answer = magical_constant * my_special_number\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b92de",
   "metadata": {},
   "source": [
    "Okay, 640, sure. Now go back to the previous code block and run it again – the one where we got the answer 149. Did you get the same answer? You should get 200 instead – it no longer gives you 149. Why? Because in the subsequent code block you redefined `my_special_number` to `10`, perhaps without even realizing.\n",
    "\n",
    "It's very easy to get into confusing situations like this because code blocks can be run in any order. If you lose track of what order you ran things in, you can quickly get in a pickle! Notebooks go against the top-to-bottom sequential flow that we normally expect when programming. Instead, the sequential flow – the order in which the blocks of code are run – exists only in your head and remains undocumented. I don't want to totally dissuade you from using notebooks (there are many good reasons to use them), but it's worth thinking about these issues if you plan to use them in the future.\n",
    "\n",
    "Finally, and more generally, please don't feel that you have to adopt all the suggestions that I've made during this course. There are many, many, many ways to do good quality science. Explore the options for yourself and find a way of working that makes sense for you, your projects, and your collaborators. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
